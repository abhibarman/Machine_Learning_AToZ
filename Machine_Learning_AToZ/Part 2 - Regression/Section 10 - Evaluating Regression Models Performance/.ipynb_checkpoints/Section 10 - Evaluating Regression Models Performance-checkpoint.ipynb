{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "R-Square :\n",
    "     R-Square tell how good is your best fitted line compared to the average line.\n",
    "     - R^2 is a goodness-of-fit [ greater the better]\n",
    "     Formula : \n",
    "          R^2 = 1 - (SSR/SST)\n",
    "          SSR  : Sum of squared residuals \n",
    "                SSR = Sum[(Y_actual - Y_Predicted)^2]\n",
    "          SST  : Sum of squared totals \n",
    "                SST = Sum[(Y_actual - Y_avg)^2]\n",
    "          Y_avg : The aveare best fit line drawn considering all the data points. \n",
    "                  Y_avg= Average(y1,y1,...yn)\n",
    "-R^2 ranges mostly between 0 to 1. Closer 1, better is the model.\n",
    "-R^2 can be negative if the best fit line fits the data worse than the average line.\n",
    "-R^2, more the better.\n",
    "\n",
    "https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/assessing-fit-least-squares-regression/v/r-squared-or-coefficient-of-determination\n",
    "\n",
    "http://statisticsbyjim.com/regression/interpret-r-squared-regression/\n",
    "\n",
    "http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Adjusted R-Squared :\n",
    "It' just like the R^2 with a penalization factor. When we add new variables, sometimes it doe not improve the model but R^2 keep on increasing which is not correct.So,R^2 is not the correct metric to decide the performance improvements of models. \n",
    "- Adj R^2 should always be increasing, if it drops,it indicate that the previous model performance was better than the current. \n",
    "\n",
    "\n",
    "When we have more than one variable,normal R^2 does not decrese i.e it does not help the model. It always increases or remains the same. So, when we have more than one vraiable & it does not help in information gain, comes the Adjusted R-squared metric.\n",
    "\n",
    "ADj R^2 = 1 - (1-R^2) [(n-1)/(n-p-1)]\n",
    "n - samples Size\n",
    "p - No. of independent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Conclusion of Part 2 - Regression\n",
    "Section 10, Lecture 80\n",
    "After learning about these six regression models, you are probably asking yourself the following questions:\n",
    "\n",
    "What are the pros and cons of each model ?\n",
    "How do I know which model to choose for my problem ?\n",
    "How can I improve each of these models ?\n",
    "Let's answer each of these questions one by one:\n",
    "\n",
    "1. What are the pros and cons of each model ?\n",
    "\n",
    "Please find here a cheat-sheet that gives you all the pros and the cons of each regression model.\n",
    "\n",
    "2. How do I know which model to choose for my problem ?\n",
    "\n",
    "First, you need to figure out whether your problem is linear or non linear. You will learn how to do that in Part 10 - Model Selection. Then:\n",
    "\n",
    "If your problem is linear, you should go for Simple Linear Regression if you only have one feature, and Multiple Linear Regression if you have several features.\n",
    "\n",
    "If your problem is non linear, you should go for Polynomial Regression, SVR, Decision Tree or Random Forest. Then which one should you choose among these four ? That you will learn in Part 10 - Model Selection. The method consists of using a very relevant technique that evaluates your models performance, called k-Fold Cross Validation, and then picking the model that shows the best results. Feel free to jump directly to Part 10 if you already want to learn how to do that.\n",
    "\n",
    "3. How can I improve each of these models ?\n",
    "\n",
    "In Part 10 - Model Selection, you will find the second section dedicated to Parameter Tuning, that will allow you to improve the performance of your models, by tuning them. You probably already noticed that each model is composed of two types of parameters:\n",
    "\n",
    "the parameters that are learnt, for example the coefficients in Linear Regression,\n",
    "the hyperparameters.\n",
    "The hyperparameters are the parameters that are not learnt and that are fixed values inside the model equations. For example, the regularization parameter lambda or the penalty parameter C are hyperparameters. So far we used the default value of these hyperparameters, and we haven't searched for their optimal value so that your model reaches even higher performance. Finding their optimal value is exactly what Parameter Tuning is about. So for those of you already interested in improving your model performance and doing some parameter tuning, feel free to jump directly to Part 10 - Model Selection.\n",
    "\n",
    "And as a BONUS, please find here some slides we made about Regularization.\n",
    "\n",
    "Now congratulations for having completed Part 2, and let's move on to the next part of the journey:\n",
    "\n",
    "Part 3 - Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
